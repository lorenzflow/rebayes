\section{ORFit: Peter's notes}

\newcommand{\B}{\vB}
\newcommand{\D}{\vD}
\newcommand{\F}{\vF}
\newcommand{\HH}{\vH}
\newcommand{\K}{\vK}
\newcommand{\Q}{\vQ}
\newcommand{\R}{\vR}
\newcommand{\normal}{\gauss}

\newtheorem{corollary}{Corollary}[theorem]

\subsection{EW-RLS Update}

We explicitly derive Equation (3) of \cite{ORFit}. Note that if we factor out the $\lambda$ dependency,
we can rewrite Equation (2) as:
\begin{align}
    w_i^{(RLS)} &= \arg\min_{w} \lambda^{i} \left( 
    \sum_{k=1}^{i} \lambda^{-k} (y_k - w^{\T} x_k)^2 + \| w - w_0 \|_{\Pi}^{2}
    \right) \\
    &= \arg\min_{w}
    \sum_{k=1}^{i} \lambda^{-k} (y_k - w^{\T} x_k)^2 + \| w - w_0 \|_{\Pi}^{2}
    \label{eqn:opt}
\end{align}

\begin{lemma}
\label{lemma:1}
    The optimization problem of Equation \ref{eqn:opt} is equivalent to
    the following Bayesian setup:
    \begin{align}
        p(w_0^{(RLS)}) &= \normal(w_0^{(RLS)} \mid w_0, \Pi^{-1}) \\
        p(w_t^{(RLS)} \mid w_{t-1}^{(RLS)}) &= \normal(w_t^{(RLS)} \mid w_{t-1}^{(RLS)}, 0) \\
        p(y_t \mid w_t^{(RLS)}, x_t) &= \normal(y_t \mid w_t^{(RLS)\T}x_t, \lambda^{t}) \label{eqn:bayes_emission}
    \end{align}
\end{lemma}

\begin{proof}
    Using Bayes' rule, we have:
    \begin{align}
        p(w \mid \{(x_k, y_k)\}_{1:i}) &\propto p(y_{1:i} \mid w, x_{1:i}) p(w) \\
        &= \prod_{k=1}^{i} \normal(y_k \mid w^{\T}x_k, \lambda^{k}) \normal(w \mid w_0, \Pi^{-1})
    \end{align}
    The negative log posterior is:
    \begin{align}
        -\log{p(w \mid \{(x_k, y_k)\}_{1:i})} &= \frac{1}{2} \sum_{k=1}^{i} \left( \frac{y_k - w^{\T} x_k}{\lambda^{\frac{k}{2}}} \right)^{2} + \frac{1}{2} (w - w_0)^{\T} \Pi (w - w_0) + \text{const} \\
        &= \frac{1}{2} \left( \sum_{k=1}^{i} \lambda^{-k} (y_k - w^{\T} x_k)^2 + \| w - w_0 \|_{\Pi}^{2} \right)
        + \text{const}
    \end{align}
    and thus the MAP solution is precisely $w$ that minimizes Equation \ref{eqn:opt}.
\end{proof}

\begin{corollary}
    Equation (3) of \cite{ORFit} holds:
    \begin{align}
        w_i^{(RLS)} &= w_{i-1}^{(RLS)} + \frac{P_{i-1}x_i}{\lambda^i + x_i^{\T}P_{i-1} x_i} (y_i - x_i^{\T} w_{i-1}^{(RLS)}) \\
        P_i &= P_{i-1} - \frac{P_{i-1} x_i x_i^{\T} P_{i-1} }{\lambda^{i} + x_i^{\T} P_{i-1} x_i}
    \end{align}
    with $w_0^{(RLS)} = w_0$ and $P_0 = \Pi^{-1}$.
    
\end{corollary}

\begin{proof}
    Applying the Kalman filter equations (Section 8.3.2 of \cite{pml2Book}), 
    with $\F_i = \Id, \HH_i = x_i^{\T}, \B_i = \D_i = \vb_i = \vd_i = \Q_i = 0, \R_i = \lambda^{i}, \vz_i = w_i^{(RLS)}, \vmu_{i|i} = w_i, \vSigma_{i|i} = P_{i}$, the predict step simplifies to:
    \begin{align}
        p(w_i^{(RLS)} \mid y_{1:i-1}) &= \normal(w_i^{(RLS)} \mid w_{i-1}, P_{i-1} )
    \end{align}
    The update step can be computed:
    \begin{align}
        \vm_i &= \HH_i \vmu_{i|i-1} = x_i^{\T} w_{i-1} \\
        \vS_i &= \HH_i \vSigma_{i|i-1} \HH_i^{\T} + \R_i = x_i^{\T} P_{i-1} x_t + \lambda^{i} \\
        \K_i &= \vSigma_{i|i-1} \HH_i^{\T} \vS_i^{-1} = \frac{P_{i-1} x_i}{\lambda^{i} + x_i^{\T} P_{i-1} x_t} \\
        \vmu_{i|i} &= w_i = \vmu_{i|i-1} + \K_i (y_i - \vm_i) = w_{i-1} + 
        \frac{P_{i-1} x_i}{\lambda^{i} + x_i^{\T} P_{i-1} x_t} (y_i - x_i^{\T} w_{i-1}) \\
        \vSigma_{i|i} &= P_i = \vSigma_{i|i-1} - \K_{i} \HH_i \vSigma_{t|t-1} 
        = P_{i-1} - \frac{P_{i-1} x_i x_i^{\T} P_{i-1} }{\lambda^{i} + x_i^{\T} P_{i-1} x_i}
    \end{align}
    Since we initialize with $p(w_0^{(RLS)}) = \normal(w_0^{(RLS)} \mid w_0, \Pi^{-1})$, the initial value for the moments are
    $w_0^{(RLS)} = w_0$ and $P_0 = \Pi^{-1}$.
\end{proof}

\subsection{Non-linear ORFit}
\label{sec:nl_orfit}
Next, consider the following setup where the emission is given by some non-linear function $f_i(w) \equiv f(x_i, w)$:
\begin{align}
    p(w_0^{(NL)}) &= \normal(w_0^{(NL)} \mid w_0, \Pi^{-1}) \\
    p(w_i^{(NL)} \mid w_{i-1}^{(NL)}) &= \normal(w_i^{(NL)} \mid w_{i-1}^{(NL)}, 0) \\
    p(y_i \mid w_i^{(NL)}, x_i) &= \normal(y_i \mid f_i(w_i), \lambda^{i})
\end{align}
Linearizing $f(\cdot)$ around the prior mean:
\begin{align}
    f_i(w_i) &= f_i(w_{i-1}) + \nabla f_i(w_{i-1}) (w_i - w_{i-1})
\end{align}
Plugging this into the likelihood:
\begin{align}
    p(y_i \mid w_i^{(NL)}, x_i) &\approx \normal(y_i \mid f_i(w_{i-1}) + \nabla f_i(w_{i-1}) (w_i - w_{i-1}), \lambda^{i})
\end{align}
Then, we can apply the standard Kalman filter equations for the fully Gaussianized system, 
with $\F_i = \Id, \HH_i = \nabla f_i(w_{i-1}), \B_i = \D_i = \vb_i = \Q_i = 0, 
\vd_i = f_i(w_{i-1}) - \nabla f_i(w_{i-1}) w_{i-1}, \R_i = \lambda^{t},
\vz_i = w_i^{(NL)}, \vmu_{i|i} = w_i, \vSigma_{i|i} = P_{i}$:
\begin{align}
    \vm_i &= \nabla f_i(w_{i-1}) w_{i-1} + f_i(w_{i-1}) - \nabla f_i(w_{i-1})w_{i-1} = f_i(w_{i-1}) \\
    \vS_i &= \nabla f_i(w_{i-1})^{\T} P_{i-1} \nabla f_i(w_{i-1}) + \lambda^{i} \\
    \K_i &= \frac{P_{i-1} \nabla f_i(w_{i-1})}{\lambda^{i}
    + \nabla f_i(w_{i-1})^{\T} P_{i-1} \nabla f_i(w_{i-1})} \\
    \vmu_{i|i} &= w_{i} = w_{i-1} + 
    \frac{P_{i-1} \nabla f_i(w_{i-1}) (y_i - f_i(w_{i-1}))}{\lambda^{i}
    + \nabla f_i(w_{i-1})^{\T} P_{i-1} \nabla f_i(w_{i-1})} \label{eqn:nl_update_mean} \\
    \vSigma_{i|i} &= P_{i} = P_{i-1} - 
    \frac{P_{i-1} \nabla f_i(w_{i-1}) \nabla f_i(w_{i-1})^{\T} P_{i-1}}{\lambda^{i}
    + \nabla f_i(w_{i-1})^{\T} P_{i-1} \nabla f_i(w_{i-1})} \label{eqn:nl_update_cov}
\end{align}
where Equations \ref{eqn:glm_update_mean} and \ref{eqn:glm_update_cov} correspond to the generalization
of the EW-RLS update equations for the general likelihood.
which exactly correspond to the NTK-RLS update
Equation (13) of \cite{ORFit}.

Letting $\lambda \rightarrow 0^{+}$, the update equations become:
\begin{align}
    w_{i} &= w_{i-1} + 
    \frac{P_{i-1} \nabla f_i(w_{i-1}) (y_i - f_i(w_{i-1}))}{\nabla f_i(w_{i-1})^{\T} P_{i-1} \nabla f_i(w_{i-1})} \label{eqn:ekf_mean} \\
    P_{i} &= P_{i-1} - 
    \frac{P_{i-1} \nabla f_i(w_{i-1}) \nabla f_i(w_{i-1})^{\T} P_{i-1}}{\nabla f_i(w_{i-1})^{\T} P_{i-1} \nabla f_i(w_{i-1})} \label{eqn:ekf_cov}
\end{align}

The ORFit algorithm (Equation (5) in \cite{ORFit}) can be applied to this system:
\begin{align}
    \tilde{g}_{i-1} &= \nabla \ell_{i}(w_{i-1}) - \sum_{v \in S_{i-1}} \proj_{v}(\nabla \ell_i (w_{i-1})) \\
    S_i &= S_{i-1} \cup \{ \nabla f_i(w_{i-1}) - \sum_{v \in S_{i-1}} \proj_{v}(\nabla f_i(w_{t-1})) \} \\
    w_i &= w_{i-1} - \eta_{i-1} \tilde{g}_{i-1} \label{eqn:orfit_update} \\
    \eta_{i-1} &= \frac{f_i(w_{i-1}) - y_i}{\nabla f_i(w_{i-1})^{\T} \tilde{g}_{i-1}} \label{eqn:orfit_lr}
\end{align}
which corresponds to Equations \ref{eqn:glm_update_mean} and \ref{eqn:glm_update_cov} with $\lambda \rightarrow 0^{+}$.


\subsection{ORFit for GLM Loss}
\label{sec:orfit_nonlinear}
Next, consider the following setup where the likelihood function is some power of some
distribution in the exponential family:
\begin{align}
    p(w_0^{(GLM)}) &= \normal(w_0^{(GLM)} \mid w_0, \Pi^{-1}) \\
    p(w_t^{(GLM)} \mid w_{t-1}^{(GLM)}) &= \normal(w_t^{(GLM)} \mid w_{t-1}^{(GLM)}, 0) \\
    p(y_t \mid w_t^{(GLM)}, x_t) &\propto \expfam(y_t \mid \vm(w_t, x_t), \vV(w_t, x_t))^{\lambda^{-t}}
\end{align}
where $\vm(\cdot), \vV(\cdot)$ are the first two moments of the likelihood function,
respectively.

Then, we can perform statistical linear regression to approximate the expfam distribution by a
Gaussian distribution with the same moments:
\begin{align}
    \expfam(y_t \mid \vm(w_t, x_t), \vV(w_t, x_t)) &\approx \normal(y_t \mid \vm(w_t, x_t), \vV(w_t, x_t))
\end{align}
Furthermore, we can linearize the moments around the prior mean:
\begin{align}
    \vm(w_t, x_t) &\approx \vm(w_{t-1}, x_t) + J_{m_t}(w_{t-1})^{\T} (w_t - w_{t-1}) \\
    \vV(w_t, x_t) &\approx \vV(w_{t-1}, x_t)
\end{align}
where $J_{m_t}(w_{t-1}) \equiv \nabla \vm(w_{t-1}, x_t)$.
Therefore, the likelihood can be approximated as:
\begin{align}
    p(y_t \mid w_t^{(GLM)}, x_t) &\approx \normal(y_t \mid \vm(w_t, x_t), \vV(w_t, x_t))^{\lambda^{-t}} \\
    &\propto \normal(y_t \mid \vm(w_{t-1}, x_t) + J_{m_t}(w_{t-1})^{\T}(w_t - w_{t-1}), \lambda^{t} \vV(w_{t-1}, x_t))
\end{align}
Then, we can apply the standard Kalman filter equations for the fully Gaussianized system, 
with $\F_i = \Id, \HH_i = J_{m_t}(w_{t-1})^{\T}, \B_i = \D_i = \vb_i = \Q_i = 0, \vd_i = \vm(w_{t-1}, x_t) - J_{m_i}(w_{t-1})^{\T}w_{t-1}, \R_i = \lambda^{t} \vV(w_{t-1}, x_t),
\vz_i = w_i^{(GLM)}, \vmu_{i|i} = w_i, \vSigma_{i|i} = P_{i}$:
\begin{align}
    \vm_i &= J_{m_t}(w_{t-1})^{\T} w_{i-1} + \vm(w_{t-1}, x_t) - J_{m_i}(w_{t-1})^{\T}w_{t-1} = \vm(w_{t-1}, x_t) \\
    \vS_i &= J_{m_i}(w_{i-1})^{\T} P_{i-1} J_{m_i}(w_{i-1}) + \lambda^{i} \vV(w_{i-1}, x_i) \\
    \K_i &= \frac{P_{i-1} J_{m_i}(w_{i-1})}{\lambda^{i} \vV(w_{i-1}, x_i)
    + J_{m_i}(w_{i-1})^{\T} P_{i-1} J_{m_i}(w_{i-1})} \\
    \vmu_{i|i} &= w_{i} = w_{i-1} + 
    \frac{P_{i-1} J_{m_i}(w_{i-1}) (y_i - \vm(w_{i-1}, x_i))}{\lambda^{i} \vV(w_{i-1}, x_i)
    + J_{m_i}(w_{i-1})^{\T} P_{i-1} J_{m_i}(w_{i-1})} \label{eqn:glm_update_mean} \\
    \vSigma_{i|i} &= P_{i} = P_{i-1} - 
    \frac{P_{i-1} J_{m_i}(w_{i-1}) J_{m_i}(w_{i-1})^{\T} P_{i-1}}{\lambda^{i} \vV(w_{i-1}, x_i)
    + J_{m_i}(w_{i-1})^{\T} P_{i-1} J_{m_i}(w_{i-1})} \label{eqn:glm_update_cov}
\end{align}
where Equations \ref{eqn:glm_update_mean} and \ref{eqn:glm_update_cov} correspond to the generalization
of the EW-RLS update equations for the general likelihood.

The ORFit algorithm (Equation (5) in \cite{ORFit}) can be applied to this system:
\begin{align}
    \tilde{g}_{i-1} &= \nabla \ell_{i}(w_{i-1}) - \sum_{v \in S_{i-1}} \proj_{v}(\nabla \ell_i (w_{i-1})) \\
    S_i &= S_{i-1} \cup \{ \nabla f_i(w_{i-1}) - \sum_{v \in S_{i-1}} \proj_{v}(\nabla f_i(w_{t-1})) \} \\
    w_i &= w_{i-1} - \eta_{i-1} \tilde{g}_{i-1} \label{eqn:orfit_update} \\
    \eta_{i-1} &= \frac{f_i(w_{i-1}) - y_i}{\nabla f_i(w_{i-1})^{\T} \tilde{g}_{i-1}} \label{eqn:orfit_lr}
\end{align}
which corresponds to Equations \ref{eqn:glm_update_mean} and \ref{eqn:glm_update_cov} with $\lambda \rightarrow 0^{+}$.

\section{Bayesian Interpretation}
\subsection{Covariance Matrix}
Recall the ORFit algorithm (Equation (5) in \cite{ORFit}):
\begin{align}
    \tilde{g}_{i-1} &= \nabla \ell_{i}(w_{i-1}) - \sum_{v \in S_{i-1}} \proj_{v}(\nabla \ell_i (w_{i-1})) \\
    S_i &= S_{i-1} \cup \{ \nabla f_i(w_{i-1}) - \sum_{v \in S_{i-1}} \proj_{v}(\nabla f_i(w_{t-1})) \} \\
    w_i &= w_{i-1} - \eta_{i-1} \tilde{g}_{i-1} \label{eqn:orfit_update} \\
    \eta_{i-1} &= \frac{f_i(w_{i-1}) - y_i}{\nabla f_i(w_{i-1})^{\T} \tilde{g}_{i-1}} \label{eqn:orfit_lr}
\end{align}

Note that using the chain rule, the loss gradient can be rewritten:
\begin{align}
    \nabla \ell_i(w_{i-1}) &= \nabla_{f_i} \ell_i(f_i(w_{i-1})) \nabla f_i(w_{i-1})
\end{align}
and therefore for a scalar loss, we can rewrite $\tilde{g}_{i-1}$ as:
\begin{align}
    \tilde{g}_{i-1} &=  \left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right) \nabla \ell_i(w_{i-1}) \\
    &= \frac{\partial \ell_i(f_i)}{\partial f_i} \left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right) \nabla f_i(w_{i-1})
\end{align}
and the ORFit update Equations \ref{eqn:orfit_update} and \ref{eqn:orfit_lr} can be expanded:
\begin{align}
    w_i &= w_{i-1} - \frac{ \frac{\partial \ell_i(f_i)}{\partial f_i} 
    \left(f_i(w_{i-1}) - y_i \right) 
    \left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right) 
    \nabla f_i(w_{i-1}) }{
    \frac{\partial \ell_i(f_i)}{\partial f_i}
    \nabla f_i(w_{i-1})^{\T} \left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right) \nabla f_i(w_{i-1})} \\
    &= w_{i-1} + \frac{\left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right) \nabla f_i(w_{i-1})
    \left(y_i - f_i(w_{i-1})\right) }{
    \nabla f_i(w_{i-1})^{\T} \left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right) \nabla f_i(w_{i-1})} \label{eqn:orfit_bayesian}
\end{align}

Comparing Equations \ref{eqn:orfit_bayesian} and \ref{eqn:ekf_mean}, we can see that the covariance $P_{i-1}$
corresponds to the projection matrix:
\begin{align}
    P_{i-1} &= I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2}
\end{align}
We can verify that the covariance update corresponds to the update of the orthogonal basis $S$ with
the newly projected gradient $v'$:
\begin{align}
    v' &= \nabla f_i(w_{i-1}) - \sum_{v \in S_{i-1}} \proj_v (\nabla f_i(w_{i-1})) \\
    &= \left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right)  \nabla f_i(w_{i-1}) \\
    &= P_{i-1} \nabla f_i(w_{i-1})
\end{align}
Plugging this into the updated covariance:
\begin{align}
    P_{i} &= I - \sum_{v \in S_{i}} \frac{v v^{\T}}{||v||^2} = I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} - 
    \frac{v'v'^{\T}}{||v'||^2} \\
    &= P_{i-1} -  \frac{v'v'^{\T}}{||v'||^2} \\
    &= P_{i-1} - \frac{P_{i-1} \nabla f_i(w_{i-1}) \nabla f_i(w_{i-1})^{\T} P_{i-1}^{\T}}{\nabla f_i(w_{i-1})^{\T} P_{i-1}^{T} P_{i-1} \nabla f_i(w_{i-1})} \\
    &= P_{i-1} - \frac{P_{i-1} \nabla f_i(w_{i-1}) \nabla f_i(w_{i-1})^{\T} P_{i-1}^{\T}}{\nabla f_i(w_{i-1})^{\T} P_{i-1} \nabla f_i(w_{i-1})} \label{eqn:proj_cov_update}
\end{align}
where the last equality follows because:
\begin{align}
    P_{i-1}^{\T}P_{i-1} &= \left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right)^{\T} 
    \left( I - \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \right) \\
    &= I - 2 \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} + \sum_{v \in S_{i-1}} \sum_{u \in S_{i-1}} 
    \frac{v v^{\T} u u^{\T}}{||v||^2||u||^{2}} \\
    &= I - 2 \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} + \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \\
    &= I + \sum_{v \in S_{i-1}} \frac{v v^{\T}}{||v||^2} \\
    &= P_{i-1}
\end{align}
Therefore, we verified that the orthogonal basis $S$ update Equation \ref{eqn:proj_cov_update}
exactly corresponds to Kalman covariance update Equation \ref{eqn:nl_update_cov} with $\lambda \to 0^{+}$.

\subsection{ORFit for GLM Loss}
\label{sec:orfit_nonlinear}
Next, consider the following setup where the likelihood function is some power of some
distribution in the exponential family:
\begin{align}
    p(w_0^{(GLM)}) &= \normal(w_0^{(GLM)} \mid w_0, \Pi^{-1}) \\
    p(w_t^{(GLM)} \mid w_{t-1}^{(GLM)}) &= \normal(w_t^{(GLM)} \mid w_{t-1}^{(GLM)}, 0) \\
    p(y_t \mid w_t^{(GLM)}, x_t) &\propto \expfam(y_t \mid \vm(w_t, x_t), \vV(w_t, x_t))^{\lambda^{-t}}
\end{align}
where $\vm(\cdot), \vV(\cdot)$ are the first two moments of the likelihood function,
respectively.

Then, we can perform statistical linear regression to approximate the expfam distribution by a
Gaussian distribution with the same moments:
\begin{align}
    \expfam(y_t \mid \vm(w_t, x_t), \vV(w_t, x_t)) &\approx \normal(y_t \mid \vm(w_t, x_t), \vV(w_t, x_t))
\end{align}
Furthermore, we can linearize the moments around the prior mean:
\begin{align}
    \vm(w_t, x_t) &\approx \vm(w_{t-1}, x_t) + J_{m_t}(w_{t-1})^{\T} (w_t - w_{t-1}) \\
    \vV(w_t, x_t) &\approx \vV(w_{t-1}, x_t)
\end{align}
where $J_{m_t}(w_{t-1}) \equiv \nabla \vm(w_{t-1}, x_t)$.
Therefore, the likelihood can be approximated as:
\begin{align}
    p(y_t \mid w_t^{(GLM)}, x_t) &\propto \normal(y_t \mid \vm(w_t, x_t), \vV(w_t, x_t))^{\lambda^{-t}} \\
    &\propto \normal(y_t \mid \vm(w_{t-1}, x_t) + J_{m_t}(w_{t-1})^{\T}(w_t - w_{t-1}), \lambda^{t} \vV(w_{t-1}, x_t))
\end{align}
Note that in the $\lambda \to 0$ limit, the likelihood becomes identical to that of the non-linear Gaussian setup:
\begin{align}
    p(y_i \mid w_i^{(GLM)}, x_i) &\approx \normal(y_i \mid \vm(w_{t-1}, x_t) + J_{m_t}(w_{t-1})^{\T}(w_t - w_{t-1}), 0)
\end{align}
and thus 


\section{Connection to Kalman Filter and L-RVGA}
\subsection{Infinite-Memory ORFit}
\begin{theorem}
\label{thm:1}
    The update steps of ORFit with infinite memory 
    (i.e. $m \geq k$, where $k$ is the size of the training set)
    are equivalent to those of a Kalman filter (or an extended 
    Kalman filter for a non-linear model) with zero dynamics 
    and emission noises.
\end{theorem}

\begin{proof}
    If the model is linear, then by Proposition 2 of \cite{ORFit},
    the update rule of ORFit is equivalent to that of the EW-RLS,
    with $\lambda = 0$ and $\Pi = I$.

    By Lemma \ref{lemma:1}, this is equivalent to the Bayesian setup:
    \begin{align}
        p(w_0^{(RLS)}) &= \normal(w_0^{(RLS)} \mid w_0, \Pi^{-1}) \\
        p(w_t^{(RLS)} \mid w_{t-1}^{(RLS)}) &= \normal(w_t^{(RLS)} \mid w_{t-1}^{(RLS)}, 0) \\
        p(y_t \mid w_t^{(RLS)}, x_t) &= \normal(y_t \mid w_t^{(RLS)\T}x_t, 0)
    \end{align}
    which is exactly the setup for a linear Kalman filter with trivial dynamics and zero noises.

    If the model is non-linear, then by Theorem 5 of \cite{ORFit}, 
    the update rule of ORFit is equivalent to that of the NTK-RLS,
    with $\lambda = 0$ and $\Pi = I$.
    As seen in Section \ref{sec:nl_orfit}, this is equivalent to the Bayesian setup:
    \begin{align}
        p(w_0^{(RLS)}) &= \normal(w_0^{(RLS)} \mid w_0, \Pi^{-1}) \\
        p(w_t^{(RLS)} \mid w_{t-1}^{(RLS)}) &= \normal(w_t^{(RLS)} \mid w_{t-1}^{(RLS)}, 0) \\
        p(y_t \mid w_t^{(RLS)}, x_t) &= \normal(y_i \mid f_i(w_{i-1}) + \nabla f_i(w_{i-1}) (w_i - w_{i-1}), 0)
    \end{align}
    which is exactly the setup for an extended Kalman filter with trivial dynamics and zero noises.
\end{proof}

Note that in the overparameterized (i.e. $p \gg k \geq i$) setting, the $i^{th}$ ORFit update
step takes $O(ip)$, which is more efficient than the $O(p^2)$ Kalman update step.

\subsection{Limited-Memory ORFit}

A limited-memory ORFit with buffer size $m$, on the other hand, is a low-rank approximation to the Kalman filter
with zero dynamics and emission noise. 

Recall that for an unlimited-memory ORFit, for the newly projected gradient $v_i = P_{i-1} \nabla f_i(w_{i-1})$,
the covariance update equation for ORFit is given by:
\begin{align}
    P_i &= P_{i-1} - \frac{v_i v_i^{\T}}{||v_i||^{2}}
\end{align}
Let $P_{i-1} = \Psi_{i-1} - W_{i-1}W_{i-1}^{\T}$ be ``diagonal minus low-rank" matrix,
where $W_{i-1} \in \mathbb{R}^{m \times p}$ is a rank-$m$ matrix. Then:
\begin{align}
     P_i &= \Psi_{i-1} - \left( W_{i-1}W_{i-1}^{\T} + \frac{v_i v_i^{\T}}{||v_i||^{2}} \right) \\
     &= \Psi_{i-1} - \left( \begin{bmatrix} W_{i-1} & \frac{v_i}{||v_i||} \end{bmatrix} \begin{bmatrix} W_{i-1} & \frac{v_i}{||v_i||} \end{bmatrix}^{\T} \right)
\end{align}
Define the matrix:
\begin{align}
    \tilde{U}_i &\equiv \begin{bmatrix} W_{i-1} & \frac{v_i}{||v_i||} \end{bmatrix} \in \mathbb{R}^{p \times (m+1)}
\end{align}
ORFit makes a rank-$m$ approximation to this matrix via SVD:\footnote{MJ: This isn't quite right, since $W_{i-1}$ and hence $\tilde{U}_i$ are both orthonormal. The singular values are stored separately in $\Sigma$, so the SVD must be applied to $\tilde{U}_i \tilde{\Sigma}_i \tilde{U}_i^{\top}$, where $\tilde{\Sigma}_i = 
\begin{bmatrix} \Sigma_{i-1} & 0 \\ 0 & \Vert v_i^{\prime}\Vert \end{bmatrix}$}
\begin{align}
    U_i &\equiv \text{top }m \text{ singular vectors in SVD}(\tilde{U}_i) \in \mathbb{R}^{p \times m}
\end{align}
Since the update does not change the diagonal matrix $\Psi_{i-1}$, it is the diagonal matrix of the
initial covariance $P_0$. Letting $P_0 = I$, then, recovers the ORFit covariance update equation:
\begin{align}
    P_i &= I - U_i U_i^{\T} = I - \sum_{v \in \col(U_i)} \frac{v v^{\T}}{||v||^2}
\end{align}

Note that instead of directly approximating the full-covariance matrix $P_i$ with
its low-rank counterpart, ORFit instead maintains the matrix $U \in \mathbb{R}^{p \times m}$. At each step $i$,
ORFit performs SVD on $\begin{bmatrix} U & P_{i-1} \nabla f_i(w_{i-1}) \end{bmatrix} \in \mathbb{R}^{p \times (m+1)}$
to form $\tilde{U} \in \mathbb{R}^{p \times m}$ with its top $m$ modes.

Then, the covariance update is given by:
\begin{align}
    P_i &= I - \sum_{v \in \col(\tilde{U})} \frac{v v^{\T}}{||v||^2}
\end{align}
Note that at each update step, we do not need to compute the full covariance 
(which would take $O(mp^2)$), since all we need to maintain the $U$ matrix is to compute:
\begin{align}
    v_i &= P_{i-1} \nabla f_i(w_{i-1}) = \left( I - \sum_{v \in \col(U_{i-1})} \frac{v v^{\T}}{||v||^2} \right) \nabla f_i(w_{i-1}) \\
    &= \nabla f_i(w_{i-1}) - \sum_{v \in \col(U_{i-1})} \frac{v \left(v^{\T}\nabla f_i(w_{i-1})\right)}{||v||^2}
\end{align}
which takes $O(mp)$.

Then, for the mean update we compute:
\begin{align}
    w_i &= w_{i-1} + \frac{v_i (y_i - f_i(w_{i-1}))}{\nabla f_i(w_{i-1})^{\T} v_i}
\end{align}
which takes just $O(p)$.

\subsection{L-RVGA}
Recall that the ORFit covariance update is:
\begin{align}
    P_i &= I - \sum_{v \in \col(\tilde{U})} \frac{v v^{\T}}{||v||^2}
\end{align}
Since $\tilde{U} \in \mathbb{R}^{p \times m}$ has linearly independent columns, we can use Gram-Schmidt to expand it to a full-rank
square matrix $U = \begin{bmatrix} \tilde{U} & v_{m+1} & \dots & v_{p} \end{bmatrix} \in \mathbb{R}^{p \times p}$.

Then since for any $x \in \mathbb{R}^{p}$, we have:
\begin{align}
    \sum_{v_i \in \col(U)} \frac{v_i v_i^{\T}}{||v_i||^2} x = \sum_{v_i \in \col(U)} \frac{v_i (v_i^{\T} x)}{||v_i||^2} =
    \sum_{v_i \in \col(U)} c_i v_i = 0 \quad \leftrightarrow \quad c_i = 0 \quad \forall i
\end{align}
and therefore the nullity of the matrix $\sum_{v_i \in \col(U)} \frac{v_i v_i^{\T}}{||v_i||^2}$ is equal to zero,
and by rank-nullity theorem, its rank is $p$.
Consider the following matrix $U$ (of rank $m$) whose columns are normalized forms of the columns of $\tilde{U}$:
\begin{align}
    U &= \begin{bmatrix} \frac{v_1}{||v_1||} & \dots & \frac{v_m}{||v_m||} \end{bmatrix} \in \mathbb{R}^{p \times m}
\end{align}
Then, the covariance update equation can be reformulated:
\begin{align}
    P_i &= I - \sum_{v \in \col(U)}v v^{\T} = I - U U^{\T}
\end{align}
Therefore, the ORFit covariance is in the form of
``low rank plus diagonal" matrix.

In particular, it is an example of a probabilistic PCA (PPCA).

The L-RVGA, on the other hand, has precision of the form:
\begin{align}
    P^{-1} &= \Psi + WW^{\T}
\end{align}
where $\Psi$ is a diagonal matrix. It takes the form of
factor analysis.

\subsection{L-RVGA}
Notice that the diagonal minus low-rank covariance matrix: $P_{i} = \Psi_i - W_i W_i^{\T}$ corresponds to
the following precision, using the Woodbury matrix identity (assuming all the relevant
matrices are invertible):
\begin{align}
    P_i^{-1} &= \left( \Psi_i - W_i W_i^{\T} \right)^{-1} \\
    &= \Psi_i^{-1} + \Psi_i^{-1} W_i 
    \left( I - W_i^{\T} \Psi_i^{-1} W_i \right) W_i^{\T} \Psi_i^{-1}
\end{align}
Suppose the term inside the parenthesis can be (Cholesky) decomposed into:
\begin{align}
    I - W_i^{\T} \Psi_i^{-1} W_i &= LL^{\T}
\end{align}
Then, the precision becomes:
\begin{align}
    P_i^{-1} &= \Psi_i^{-1} + \left( \Psi_i^{-1} W_i L \right) \left( \Psi_i^{-1} W_i L \right)^{\T}
\end{align}
Note that since $\Psi_i^{-1}$ is an (invertible) diagonal
matrix, it has full rank, and therefore $(\Psi_i^{-1}W_i)$ has rank $m$. Note also that:
\begin{align}
    \rank(\Psi_i^{-1} W_i L) &\leq \min(\rank(\Psi_i^{-1} W_i), L) \leq m
\end{align}
and therefore, the corresponding precision $P_i^{-1}$ for the diagonal minus low-rank covariance matrix $P_i$
has a ``diagonal plus low-rank" form.

While ORFit is making the diagonal minus low-rank covariance approximation via SVD, 
L-RVGA is making the diagonal plus low-rank precision approximation via EM.

