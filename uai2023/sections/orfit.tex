
\section{ORFit}

\subsection{ORFit Algorithm}

\begin{algorithm}
\dontprintsemicolon
\caption{Simplified ORFit}
\label{algo:orfit}
Input: Data sequence $((x_k, y_k))_{k=1}^{K}$, memory limit $m$, prior $w_0$.\\
Output: The optimal parameter $w \in \mathbb{R}^{p}$ \\
Initialize $U \gets [0]^{p \times m}$, $\Sigma \gets [0]^{p}$, $w \gets w_0$ \\
\For{$i=1,2,3, \dots$}{
$v' \gets \nabla f_i(w) - \sum_{v \in \col(U)} \proj_{v}(\nabla f_i(w))$ \\
\If{$||v'|| > \min(\Sigma)$} {
    $U[:, \arg\min(\Sigma)] \gets \frac{v'}{||v'||}$ \\
    $\Sigma[\arg\min(\Sigma)] \gets ||v'||$
}
$w \gets w - \frac{(f_i(w) - y_i) v'}{\nabla f_i(w)^{\T} v'}$
}
\end{algorithm}

Note that the update step for ORFit takes $O(mp)$, which is more efficient than standard (E)KF's update step which takes $O(p^2)$.

\subsection{Posterior Predictive Covariance}

Let $F$ be the linearized model, $\Sigma$ be the posterior covariance of ORFit, 
and $P$ be the posterior predictive covariance.
% \begin{align}
%     P &= F \Sigma F^{\T} = F (I - UU^{\T}) F^{\T} \\
%     &= FF^{\T} - (FU) (FU)^{\T} 
%     \label{eq:ORFit-PPD-projective}
% \end{align}
Note that $FF^{\T}$ takes time $O(p)$, $FU$ takes time $O(mp)$ and $(FU)(FU)^{\T}$ takes time $O(m)$,
so the computation of posterior covariance takes time $O(mp)$.


%%%%%%%%%%%
% Copied from orfit-sampling.tex

\subsection{A different connection to EKF and L-RVGA}

The preceding correspondence between ORFit and EKF relies on the assumption of zero observation noise ($\lambda=0$). This assumption entails infinite posterior precision in the direction of the gradient. Together with the linearization of the model and the assumption of no dynamics, the $\lambda=0$ assumption is key to deriving the orthogonal projection for the mean update. If instead we maintain positive observation noise, we obtain a different interpretation of ORFit's relationship to the EKF, which in turn suggests a more flexible and Bayesian variation of the algorithm.

Consider the following Bayesian setup:
\begin{align}
    p\left(w_{0}\right)	&= \mathcal{N}\left(w_{0}\vert \mu_{0},\mathbb{I}_{p}\right) \\
    p\left(w_{i}\vert w_{i-1}\right) &= \mathcal{N}\left(w_{i}\vert w_{i-1},0\right) \\
    p\left(y_{i}\vert w_{i},x_{i}\right) &= \mathcal{N}\left(y_{i}\vert f_{i}\left(w_{i-1}\right)+\nabla f_{i}\left(w_{i-1}\right)^{\top}\left(w_{i}-w_{i-1}\right),\lambda\right)
\end{align} 
Standard EKF equations yield
\begin{align}
p\left(w_{i}\vert\mathcal{D}_{1:i}\right)=\mathcal{N}\left(w_{i}\vert\mu_{i},S_{i}\right)
\end{align} 
with 
\begin{align}
S_{i}^{-1}	&= \mathbb{I}_{p}+\lambda^{-1}\sum_{k=1}^{i}\nabla f_{k}\left(\mu_{k-1}\right)\nabla f_{k}\left(\mu_{k-1}\right)^{\top} \\
	&= \mathbb{I}_{p}+\lambda^{-1}W_{i}W_{i}^{\top}
\end{align} 
 where
\begin{align}
 W_{i}=\left[\nabla f_{k}\left(w_{k-1}\right)\right]_{1\le k\le i}^{\top}
\end{align} 
 is the matrix of past gradients.

ORFit's update rules, specifically for $v_{i},U,\Sigma$, can be interpreted as maintaining an approximation of the top $m$ singular vectors and values of $W_{i}$:
\begin{align}
U_{i}\Sigma_{i}	={\rm SVD}_{m}\left(W_{i}\right)
\end{align}
Specifically, consider the following “incremental SVD” algorithm, which at each time step appends the current gradient to the existing rank-$m$ approximation and recomputes the SVD:
\begin{align}
\tilde{W}_{i}	&= \left[\begin{array}{cc} U_{i-1}\Sigma_{i-1} & \nabla f_{i}\left(\mu_{i-1}\right)\end{array}\right] \label{eq:W-tilde}\\
U_{i}\Sigma_{i}	&= {\rm SVD}_{m}\left(\tilde{W}_{i}\right)
\end{align}
This problem is not efficiently computable (as far as we can tell), because the new gradient will generally be oblique to the existing vectors. ORFit makes the problem tractable by replacing it with
\begin{align}
    v_{i}	&=\left(\mathbb{I}_{p}-U_{i-1}U_{i-1}^{\top}\right)\nabla f_{i}\left(\mu_{i-1}\right) \\
    \ring{W}_{i}	&=\left[\begin{array}{cc} U_{i-1}\Sigma_{i-1} & v_{i}\end{array}\right] \\
    U_{i}\Sigma_{i}	&={\rm SVD}_{m}\left(\ring{W}_{i}\right)
\end{align}
which is trivial to solve.

Under this interpretation, ORFit approximates the EKF via
\begin{align}
    S_{i}^{-1}=\mathbb{I}_{p}+\lambda^{-1}U_{i}\Sigma_{i}^{2}U_{i}^{\top}
\end{align}
This distribution can be efficiently sampled as described below in Eq \eqref{eq:ORFit-PPD-sample-b}.
It yields a different expression for the posterior predictive variance from that in Eq \eqref{eq:ORFit-PPD-projective}:
\begin{align}
P_{i}	&=\nabla f_{i}\left(\mu_{i-1}\right)^{\top}S_{i}^{-1}\nabla f_{i}\left(\mu_{i-1}\right) \\
	&=\nabla f_{i}\left(\mu_{i-1}\right)^{\top}\left(\mathbb{I}_{p}+\lambda^{-1}U_{i}\Sigma_{i}^{2}U_{i}^{\top}\right)^{-1}\nabla f_{i}\left(\mu_{i-1}\right)
 \end{align} 
It also suggests a different update for the mean that is more Bayesian than ORFit's: 
\begin{align}
    \mu_{i}	&=\arg\min_{w}\left\{\lambda^{-1}\left(f_{i}\left(\mu_{i-1}\right)+\nabla f_{i}\left(\mu_{i-1}\right)^{\top}\left(w-\mu_{i-1}\right)-y_{i}\right)^{2}+\left(w-\mu_{i-1}\right)^{\top}S_{i-1}^{-1}\left(w-\mu_{i-1}\right)\right\} \\
    &=\mu_{i-1}+\left(\nabla f_{i}\left(\mu_{i-1}\right)\nabla f_{i}\left(\mu_{i-1}\right)^{\top}+\lambda S_{i-1}^{-1}\right)^{-1}\nabla f_{i}\left(\mu_{i-1}\right)\left(y_{i}-f_{i}\left(\mu_{i-1}\right)\right) \\
    &= \mu_{i-1} + \lambda^{-1}\left(\mathbb{I}_p - \tilde{W}_i\left(\lambda\mathbb{I}_{m+1}+\tilde{W}_i^{\top}\tilde{W}_i\right)^{-1}\tilde{W}_i^{\top}\right) \nabla f_{i}\left(\mu_{i-1}\right)\left(y_{i}-f_{i}\left(\mu_{i-1}\right)\right)
\end{align}
where $\tilde{W}_i$ is as defined in Eq \eqref{eq:W-tilde}.

\subsection{ORFit sampling}

The Orthogonal Recursive Fitting (ORFit \cite{ORFit}) algorithm maintains a low-rank approximation of the posterior precision matrix, stored as $U_t\Sigma_t^2 U_t^{\top}$, where $U_t\in\mathbb{R}^{p\times m}$ with $U_t^{\top} U_t=\mathbb{I}_m$, and $\Sigma_t\in\mathbb{R}^{m\times m}$ is diagonal ($p$ is the parameter dimension and $m$ is the rank of the approximation).

Sampling from ORFit's posterior is complicated by the fact that it is improper, having infinite variance in the subspace $V_t$ orthogonal to $\text{colspan}(U_t)$. We approximate with a proper posterior by imposing finite variance on $V_t$, in one of two ways.

First, we can sample from 
\begin{align}
    \gauss(\mu_t,(U_t \Sigma^2_t U_t^{\top})^+ + \lambda^{-2}(\mathbb{I}_p-UU^{\top})) \label{eq:ORFit-approx-a}
\end{align}
where $A^+$ denotes the pseudo-inverse of $A$, and $\lambda$ is a hyperparameter determining the variance along $V_t$. The orthonormality property of $U_t$ implies 
\begin{align}
    (U_t \Sigma_t^2 U_t^{\top})^+ = U_t \Sigma_t^{-2} U_t^{\top}.
\end{align}
Thus the pseudo-inverse contributes zero variance along $V_t$. Sampling
\begin{align}
    z &\sim \gauss(0,\mathbb{I}_m) \\
    \zeta &\sim \gauss(0,\mathbb{I}_p),
\end{align}
we can define
\begin{align}
    y = \mu_t + U_t\Sigma_t^{-1} z + \lambda^{-1} (\mathbb{I}_p - U_t U_t^{\top}) \zeta,
\end{align}
with the second term computed efficiently as $\lambda^{-1}(\zeta - U_t (U_t^{\top} \zeta))$. It is easily verified that $y$ has the distribution in \eqref{eq:ORFit-approx-a}. 

Second, we can add some precision to all dimensions, sampling from
\begin{align}
    \gauss(\mu_t,(U_t\Sigma_t^2U_t^{\top} + \lambda^2\mathbb{I}_p)^{-1}).
\end{align}
Leveraging a result from \citep{LRVGA}, sample $z$ and $\zeta$ as above, and define
\begin{align}
    y = \mu_t + \lambda^{-1}\left(\mathbb{I}_p - U_t\left(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m\right)^{-1}U_t^{\top}\right)\zeta + U_t\Sigma_t^{-1}\left(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m\right)^{-1}z.
    \label{eq:ORFit-PPD-sample-b}
\end{align}
Then we have $\mathbb{E}[y]=\mu_t$ and
\begin{align}
    \mathbb{E}[yy^{\top}] &= \lambda^{-2}\left(\mathbb{I}_p - U_t\left(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m\right)^{-1}U_t^{\top}\right)^2 + U_t\Sigma_t^{-1}\left(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m\right)^{-2}\Sigma_t^{-1}U_t^{\top} \\
    % &= \lambda^{-2}\mathbb{I}_p - 2\lambda^{-2}U_t(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-1}U_t^{\top} + \lambda^{-2}U_t(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-2}U_t^{\top} + U_t\Sigma_t^{-1}(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-2}\Sigma_t^{-1}U_t^{\top} \\
    % &= \lambda^{-2}\mathbb{I}_p + U_t\left(- 2\lambda^{-2}(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-1} + \lambda^{-2}(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-2} + \Sigma_t^{-1}(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-2}\Sigma_t^{-1}\right)U_t^{\top} \\    
    % &= \lambda^{-2}\mathbb{I}_p + \lambda^{-2}U_t\left(-2(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-1} + (\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-2} + \lambda^2\Sigma_t^{-2}(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m)^{-2}\right)U_t^{\top} \\
    % &= \lambda^{-2}\mathbb{I}_p - \lambda^{-2}U_t\left(\lambda^2\Sigma_t^{-2}+\mathbb{I}_m\right)^{-1}U_t^{\top} \\
    &= \lambda^{-2}\mathbb{I}_p - \lambda^{-2}U_t\left(\Sigma_t^{-2}+\lambda^{-2}U_t^{\top}U_t\right)^{-1}U_t^{\top}\lambda^{-2} \\
    &= \left( U_t\Sigma_t^2U_t^{\top} + \lambda^2\mathbb{I}_p \right)^{-1}
\end{align}
as desired.
